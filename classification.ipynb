{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c716d9ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, balanced_accuracy_score, f1_score\n",
    "import xgboost as xgb\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# Prepare features and target\n",
    "# ------------------------------------------------------------------\n",
    "X = df.loc[:, 'BIC_TAI':'BRD_Avg_Echo']\n",
    "y = df['Brooke'].astype(int)\n",
    "subjects = df['subject']\n",
    "\n",
    "valid_mask = ~(subjects.isna() | y.isna())\n",
    "X = X[valid_mask]\n",
    "y = y[valid_mask]\n",
    "subjects = subjects[valid_mask]\n",
    "\n",
    "print(f\"X shape: {X.shape}\")\n",
    "print(f\"y shape: {y.shape}\")\n",
    "print(f\"Subjects: {subjects.nunique()}\")\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# Classification models\n",
    "# ------------------------------------------------------------------\n",
    "models = {\n",
    "    'KNN': KNeighborsClassifier(n_neighbors=5, n_jobs=20),\n",
    "\n",
    "    'SVM': SVC(\n",
    "        kernel='rbf',\n",
    "        C=1.0,\n",
    "        gamma='scale',\n",
    "        probability=True,\n",
    "        random_state=42\n",
    "    ),\n",
    "\n",
    "    'Random Forest': RandomForestClassifier(\n",
    "        n_estimators=200,\n",
    "        random_state=42,\n",
    "        n_jobs=20\n",
    "    ),\n",
    "\n",
    "    'XGBoost': xgb.XGBClassifier(\n",
    "        objective='multi:softprob',\n",
    "        eval_metric='mlogloss',\n",
    "        random_state=42,\n",
    "        n_jobs=20\n",
    "    ),\n",
    "\n",
    "    'Logistic Regression': LogisticRegression(\n",
    "        penalty='l2',\n",
    "        max_iter=1000,\n",
    "        multi_class='auto'\n",
    "    ),\n",
    "\n",
    "    'MLP': MLPClassifier(\n",
    "        hidden_layer_sizes=(100, 50),\n",
    "        activation='relu',\n",
    "        solver='adam',\n",
    "        alpha=0.001,\n",
    "        learning_rate_init=0.001,\n",
    "        max_iter=1000,\n",
    "        early_stopping=True,\n",
    "        validation_fraction=0.1,\n",
    "        n_iter_no_change=10,\n",
    "        random_state=42\n",
    "    )\n",
    "}\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# Metric function\n",
    "# ------------------------------------------------------------------\n",
    "def calculate_metrics(y_true, y_pred):\n",
    "    return {\n",
    "        'accuracy': accuracy_score(y_true, y_pred),\n",
    "        'balanced_accuracy': balanced_accuracy_score(y_true, y_pred),\n",
    "        'f1_macro': f1_score(y_true, y_pred, average='macro')\n",
    "    }\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# LOSO Cross-Validation (Classification)\n",
    "# ------------------------------------------------------------------\n",
    "def loso_cv_classification(X, y, subjects, models):\n",
    "    unique_subjects = subjects.unique()\n",
    "\n",
    "    all_preds = {\n",
    "        m: {'y_true': [], 'y_pred': []}\n",
    "        for m in models\n",
    "    }\n",
    "\n",
    "    for i, test_subject in enumerate(unique_subjects):\n",
    "        print(f\"Subject {test_subject} ({i+1}/{len(unique_subjects)})\")\n",
    "\n",
    "        train_mask = subjects != test_subject\n",
    "        test_mask = subjects == test_subject\n",
    "\n",
    "        X_train, X_test = X[train_mask], X[test_mask]\n",
    "        y_train, y_test = y[train_mask], y[test_mask]\n",
    "\n",
    "        if len(y_test) < 1:\n",
    "            continue\n",
    "\n",
    "        scaler = StandardScaler()\n",
    "        X_train = scaler.fit_transform(X_train)\n",
    "        X_test = scaler.transform(X_test)\n",
    "\n",
    "        for name, model in models.items():\n",
    "            try:\n",
    "                model.fit(X_train, y_train)\n",
    "                y_pred = model.predict(X_test)\n",
    "\n",
    "                all_preds[name]['y_true'].extend(y_test.tolist())\n",
    "                all_preds[name]['y_pred'].extend(y_pred.tolist())\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"  {name} failed: {e}\")\n",
    "\n",
    "    results = {}\n",
    "    for name, preds in all_preds.items():\n",
    "        if len(preds['y_true']) == 0:\n",
    "            continue\n",
    "\n",
    "        y_true = np.array(preds['y_true'])\n",
    "        y_pred = np.array(preds['y_pred'])\n",
    "\n",
    "        metrics = calculate_metrics(y_true, y_pred)\n",
    "        metrics['n_samples'] = len(y_true)\n",
    "        metrics['y_true'] = y_true\n",
    "        metrics['y_pred'] = y_pred\n",
    "\n",
    "        results[name] = metrics\n",
    "\n",
    "    return results\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# Run LOSO\n",
    "# ------------------------------------------------------------------\n",
    "results = loso_cv_classification(X, y, subjects, models)\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# Summary table\n",
    "# ------------------------------------------------------------------\n",
    "summary_df = pd.DataFrame([\n",
    "    {\n",
    "        'Model': name,\n",
    "        'Accuracy': res['accuracy'],\n",
    "        'Balanced_Accuracy': res['balanced_accuracy'],\n",
    "        'F1_macro': res['f1_macro'],\n",
    "        'N_samples': res['n_samples']\n",
    "    }\n",
    "    for name, res in results.items()\n",
    "])\n",
    "\n",
    "print(\"\\nCLASSIFICATION RESULTS (LOSO, Combined Predictions)\")\n",
    "print(summary_df.round(4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db8313f5-cc73-4795-a896-c8b4aea1d9a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import Ridge, Lasso\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from scipy.stats import spearmanr\n",
    "import xgboost as xgb\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Prepare features and target - FIXED\n",
    "X = df.loc[:, 'BIC_TAI':'BRD_Avg_Echo']  # First 9 columns as features (actual data, not column names)\n",
    "y = df['Brooke']\n",
    "subjects = df['subject']\n",
    "\n",
    "# Remove rows where subject or target is NaN\n",
    "valid_mask = ~(subjects.isna() | y.isna())\n",
    "X = X[valid_mask]\n",
    "y = y[valid_mask]\n",
    "subjects = subjects[valid_mask]\n",
    "\n",
    "print(f\"After removing NaN values:\")\n",
    "print(f\"X shape: {X.shape}\")\n",
    "print(f\"y shape: {y.shape}\")\n",
    "print(f\"subjects shape: {subjects.shape}\")\n",
    "\n",
    "# Initialize models with 20 core CPU utilization\n",
    "models = {\n",
    "    'KNN': KNeighborsRegressor(n_neighbors=5, n_jobs=20),\n",
    "    'SVM': SVR(kernel='rbf', C=1.0, gamma='scale'),  # SVM doesn't support n_jobs\n",
    "    'Random Forest': RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=20),\n",
    "    'XGBoost': xgb.XGBRegressor(random_state=42, verbosity=0, n_jobs=20),\n",
    "    'Ridge': Ridge(alpha=1.0),  # Ridge doesn't support n_jobs\n",
    "    'Lasso': Lasso(alpha=0.1, max_iter=1000),  # Lasso doesn't support n_jobs\n",
    "    'MLP': MLPRegressor(\n",
    "        hidden_layer_sizes=(100, 50),  # Two hidden layers with 100 and 50 neurons\n",
    "        activation='relu',\n",
    "        solver='adam',\n",
    "        alpha=0.001,  # L2 regularization\n",
    "        batch_size='auto',\n",
    "        learning_rate='constant',\n",
    "        learning_rate_init=0.001,\n",
    "        max_iter=1000,\n",
    "        random_state=42,\n",
    "        early_stopping=True,\n",
    "        validation_fraction=0.1,\n",
    "        n_iter_no_change=10,\n",
    "        tol=1e-4\n",
    "    )\n",
    "}\n",
    "\n",
    "# Function to calculate performance metrics\n",
    "def calculate_metrics(y_true, y_pred):\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    \n",
    "    # Spearman correlation with p-value\n",
    "    correlation, p_value = spearmanr(y_true, y_pred)\n",
    "    \n",
    "    return mae, mse, correlation, p_value\n",
    "\n",
    "# Leave-One-Subject-Out Cross-Validation with Combined Predictions\n",
    "def loso_cv_combined(X, y, subjects, models):\n",
    "    unique_subjects = subjects.dropna().unique()\n",
    "    \n",
    "    # Store all predictions and true values for each model\n",
    "    all_predictions = {model_name: {'y_true': [], 'y_pred': []} \n",
    "                      for model_name in models.keys()}\n",
    "    \n",
    "    print(f\"Performing Leave-One-Subject-Out CV with {len(unique_subjects)} subjects...\")\n",
    "    \n",
    "    for i, test_subject in enumerate(unique_subjects):\n",
    "        print(f\"Processing subject {test_subject} ({i+1}/{len(unique_subjects)})\")\n",
    "        \n",
    "        # Split data\n",
    "        train_mask = subjects != test_subject\n",
    "        test_mask = subjects == test_subject\n",
    "        \n",
    "        X_train, X_test = X[train_mask], X[test_mask]\n",
    "        y_train, y_test = y[train_mask], y[test_mask]\n",
    "        \n",
    "        # Skip if test set is too small\n",
    "        if len(y_test) < 1:\n",
    "            print(f\"  Skipping subject {test_subject} - insufficient data ({len(y_test)} samples)\")\n",
    "            continue\n",
    "            \n",
    "        # Standardize features\n",
    "        scaler = StandardScaler()\n",
    "        X_train_scaled = scaler.fit_transform(X_train)\n",
    "        X_test_scaled = scaler.transform(X_test)\n",
    "        \n",
    "        # Train and evaluate each model\n",
    "        for model_name, model in models.items():\n",
    "            try:\n",
    "                # Train model\n",
    "                model.fit(X_train_scaled, y_train)\n",
    "                \n",
    "                # Make predictions\n",
    "                y_pred = model.predict(X_test_scaled)\n",
    "                \n",
    "                # Store predictions and true values\n",
    "                all_predictions[model_name]['y_true'].extend(y_test.tolist())\n",
    "                all_predictions[model_name]['y_pred'].extend(y_pred.tolist())\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"  Error with {model_name} for subject {test_subject}: {e}\")\n",
    "                continue\n",
    "    \n",
    "    # Calculate overall performance metrics for each model\n",
    "    results = {}\n",
    "    for model_name in models.keys():\n",
    "        if len(all_predictions[model_name]['y_true']) > 0:\n",
    "            y_true_all = np.array(all_predictions[model_name]['y_true'])\n",
    "            y_pred_all = np.array(all_predictions[model_name]['y_pred'])\n",
    "            \n",
    "            mae, mse, correlation, p_value = calculate_metrics(y_true_all, y_pred_all)\n",
    "            \n",
    "            results[model_name] = {\n",
    "                'mae': mae,\n",
    "                'mse': mse,\n",
    "                'correlation': correlation,\n",
    "                'p_value': p_value,\n",
    "                'n_samples': len(y_true_all),\n",
    "                'y_true': y_true_all,\n",
    "                'y_pred': y_pred_all\n",
    "            }\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Run LOSO CV with combined predictions\n",
    "results = loso_cv_combined(X, y, subjects, models)\n",
    "\n",
    "# Create summary DataFrame\n",
    "def create_summary_df(results):\n",
    "    summary_data = []\n",
    "    \n",
    "    for model_name, metrics in results.items():\n",
    "        summary_data.append({\n",
    "            'Model': model_name,\n",
    "            'MAE': metrics['mae'],\n",
    "            'MSE': metrics['mse'],\n",
    "            'RMSE': np.sqrt(metrics['mse']),\n",
    "            'Correlation': metrics['correlation'],\n",
    "            'P_value': metrics['p_value'],\n",
    "            'N_samples': metrics['n_samples']\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(summary_data)\n",
    "\n",
    "# Generate summary\n",
    "summary_df = create_summary_df(results)\n",
    "\n",
    "# Display results\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"LEAVE-ONE-SUBJECT-OUT CROSS-VALIDATION RESULTS\")\n",
    "print(\"(Combined predictions across all subjects)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nDataset Info:\")\n",
    "print(f\"- Total samples: {len(X)}\")\n",
    "print(f\"- Features: {X.shape[1]}\")\n",
    "print(f\"- Subjects: {len(subjects.unique())}\")\n",
    "print(f\"- Target variable: Brooke\")\n",
    "\n",
    "print(f\"\\nOverall Performance (All Predictions Combined):\")\n",
    "print(summary_df.round(4))\n",
    "\n",
    "# Detailed results for each model\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"DETAILED RESULTS BY MODEL\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for model_name in models.keys():\n",
    "    if model_name in results:\n",
    "        print(f\"\\n{model_name}:\")\n",
    "        print(f\"  MAE: {results[model_name]['mae']:.4f}\")\n",
    "        print(f\"  MSE: {results[model_name]['mse']:.4f}\")\n",
    "        print(f\"  RMSE: {np.sqrt(results[model_name]['mse']):.4f}\")\n",
    "        print(f\"  Spearman Correlation: {results[model_name]['correlation']:.4f}\")\n",
    "        print(f\"  P-value: {results[model_name]['p_value']:.4f}\")\n",
    "        print(f\"  Total predictions: {results[model_name]['n_samples']}\")\n",
    "\n",
    "# Find best model for each metric\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"BEST MODELS BY METRIC\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "if not summary_df.empty:\n",
    "    best_mae = summary_df.loc[summary_df['MAE'].idxmin(), 'Model']\n",
    "    best_mse = summary_df.loc[summary_df['MSE'].idxmin(), 'Model']\n",
    "    best_rmse = summary_df.loc[summary_df['RMSE'].idxmin(), 'Model']\n",
    "    best_corr = summary_df.loc[summary_df['Correlation'].idxmax(), 'Model']\n",
    "    \n",
    "    print(f\"Best MAE: {best_mae} ({summary_df.loc[summary_df['MAE'].idxmin(), 'MAE']:.4f})\")\n",
    "    print(f\"Best MSE: {best_mse} ({summary_df.loc[summary_df['MSE'].idxmin(), 'MSE']:.4f})\")\n",
    "    print(f\"Best RMSE: {best_rmse} ({summary_df.loc[summary_df['RMSE'].idxmin(), 'RMSE']:.4f})\")\n",
    "    print(f\"Best Correlation: {best_corr} ({summary_df.loc[summary_df['Correlation'].idxmax(), 'Correlation']:.4f})\")\n",
    "\n",
    "# Statistical significance analysis\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STATISTICAL SIGNIFICANCE ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for model_name in models.keys():\n",
    "    if model_name in results:\n",
    "        p_value = results[model_name]['p_value']\n",
    "        significance = \"significant\" if p_value < 0.05 else \"not significant\"\n",
    "        print(f\"{model_name}: p = {p_value:.4f} ({significance})\")\n",
    "\n",
    "print(\"\\nAnalysis complete!\")\n",
    "\n",
    "# Optional: Save predictions for further analysis\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PREDICTION STORAGE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"All predictions are stored in the 'results' dictionary.\")\n",
    "print(\"Access them using: results['ModelName']['y_true'] and results['ModelName']['y_pred']\")\n",
    "print(\"Example: results['Random Forest']['y_true'] contains all true values\")\n",
    "print(\"Example: results['Random Forest']['y_pred'] contains all predicted values\")\n",
    "\n",
    "# MLP specific information\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"MLP MODEL CONFIGURATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"MLP Configuration:\")\n",
    "print(\"- Hidden layers: (100, 50) - Two hidden layers with 100 and 50 neurons\")\n",
    "print(\"- Activation: ReLU\")\n",
    "print(\"- Solver: Adam optimizer\")\n",
    "print(\"- Learning rate: 0.001\")\n",
    "print(\"- L2 regularization (alpha): 0.001\")\n",
    "print(\"- Early stopping: Enabled\")\n",
    "print(\"- Max iterations: 1000\")\n",
    "print(\"- Tolerance: 1e-4\")\n",
    "print(\"\\nNote: MLP may take longer to train compared to other models.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6c50da86-1e51-4cde-b592-ae92eb467062",
   "metadata": {},
   "outputs": [],
   "source": [
    "# summary_df.drop(columns=['N_samples'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb109266-ec90-44dc-a5a3-d4216c7f5f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9cc4ef5a-159e-477c-8ce0-73bf5eae2552",
   "metadata": {},
   "outputs": [],
   "source": [
    "# summary_df = summary_df.drop(columns=['R²'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f287614a-b9a8-4528-8f84-11679f110621",
   "metadata": {},
   "outputs": [],
   "source": [
    "# results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8b62990a-f407-433d-8dd7-8bab03279d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# results['KNN']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b81d7dc1-5d05-4f62-94df-ee1b1f92e1f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# Calculate R² for each model\n",
    "r2_scores = []\n",
    "for model_name in results.keys():\n",
    "    y_true = results[model_name]['y_true']\n",
    "    y_pred = results[model_name]['y_pred']\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    r2_scores.append(r2)\n",
    "\n",
    "# Add R² column to summary_df\n",
    "summary_df['R²'] = r2_scores\n",
    "\n",
    "# Drop N_samples column\n",
    "summary_df = summary_df.drop('N_samples', axis=1)\n",
    "\n",
    "# Reorder columns for better presentation\n",
    "summary_df = summary_df[['Model', 'MAE', 'MSE', 'RMSE', 'R²', 'Correlation', 'P_value']]\n",
    "\n",
    "print(\"Updated summary with R² scores:\")\n",
    "print(summary_df)\n",
    "\n",
    "# Optional: Sort by R² (descending) to see best performing models\n",
    "print(\"\\nSummary sorted by R² (best to worst):\")\n",
    "summary_sorted = summary_df.sort_values('R²', ascending=False).reset_index(drop=True)\n",
    "summary_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c01f4497-0488-4032-974a-104c47ce6676",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_sorted.to_csv('results.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05f85d0b-1e7d-4288-bee6-ffe38a6ab571",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import r2_score\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "# Calculate the required metrics for each model\n",
    "summary_data = []\n",
    "\n",
    "for model_name in results.keys():\n",
    "    y_true = results[model_name]['y_true']\n",
    "    y_pred = results[model_name]['y_pred']\n",
    "    \n",
    "    # Calculate RMSE\n",
    "    rmse = np.sqrt(np.mean((y_true - y_pred)**2))\n",
    "    \n",
    "    # Calculate NRMSE (Normalized RMSE)\n",
    "    # Using range normalization: NRMSE = RMSE / (max(y_true) - min(y_true))\n",
    "    nrmse = rmse / (np.max(y_true) - np.min(y_true))\n",
    "    \n",
    "    # Calculate R²\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    \n",
    "    # Calculate Correlation and p-value\n",
    "    correlation, p_value = pearsonr(y_true, y_pred)\n",
    "    \n",
    "    # Store results\n",
    "    summary_data.append({\n",
    "        'Model': model_name,\n",
    "        'RMSE': rmse,\n",
    "        'NRMSE': nrmse,\n",
    "        'R²': r2,\n",
    "        'Correlation': correlation,\n",
    "        'P_value': p_value\n",
    "    })\n",
    "\n",
    "# Create new summary dataframe\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "\n",
    "print(\"Summary with selected metrics:\")\n",
    "print(summary_df)\n",
    "\n",
    "# Optional: Sort by R² (descending) to see best performing models\n",
    "print(\"\\nSummary sorted by R² (best to worst):\")\n",
    "summary_sorted = summary_df.sort_values('R²', ascending=False).reset_index(drop=True)\n",
    "print(summary_sorted)\n",
    "\n",
    "# Optional: Display with formatted numbers for better readability\n",
    "print(\"\\nFormatted summary:\")\n",
    "summary_formatted = summary_df.copy()\n",
    "summary_formatted['RMSE'] = summary_formatted['RMSE'].round(6)\n",
    "summary_formatted['NRMSE'] = summary_formatted['NRMSE'].round(6)\n",
    "summary_formatted['R²'] = summary_formatted['R²'].round(6)\n",
    "summary_formatted['Correlation'] = summary_formatted['Correlation'].round(6)\n",
    "summary_formatted['P_value'] = summary_formatted['P_value'].round(6)\n",
    "summary_formatted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "32711fa9-906c-4eba-b702-d231d9c89b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_formatted.to_csv('results.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2a2d1a18-f4e5-4469-b609-36e3f8c85944",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('results.csv')\n",
    "results_df = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80841f0f-0667-4b8a-b668-7da5c34c68fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f59cae88-bfe5-461f-b511-a15938649915",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
