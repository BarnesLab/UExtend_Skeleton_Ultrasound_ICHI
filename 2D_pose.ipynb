{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5d65a2be-ab8c-450c-9be5-1afb58b83636",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "from datetime import timedelta\n",
    "from datetime import datetime\n",
    "import os\n",
    "import subprocess\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "22c3254a-0d37-44a3-888c-95c841770143",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch file: /mnt/sdb/arafat/.local/lib/python3.10/site-packages/torch/__init__.py\n",
      "torch version: 2.5.1+cu121\n",
      "torch.version.cuda: 12.1\n",
      "numpy: 2.2.6\n",
      "cuda_available: True\n",
      "device: NVIDIA RTX A6000\n"
     ]
    }
   ],
   "source": [
    "import torch, numpy\n",
    "print(\"torch file:\", torch.__file__)\n",
    "print(\"torch version:\", torch.__version__)\n",
    "print(\"torch.version.cuda:\", torch.version.cuda)\n",
    "print(\"numpy:\", numpy.__version__)\n",
    "print(\"cuda_available:\", torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"device:\", torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9826aeef-9371-42d1-8c92-24a04691201c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Group\n",
       "DMD        28\n",
       "Healthy    23\n",
       "SMA        12\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brooke_df = pd.read_csv('brooke_df.csv')\n",
    "df1 = brooke_df.drop_duplicates(subset=['Subj#'], keep='first').reset_index(drop=True)\n",
    "df1['Group'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c1fbab6a-192c-41ce-989e-494f4b035b48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Subj#</th>\n",
       "      <th>Visit #</th>\n",
       "      <th>Age @ \\nenrollment</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Dom</th>\n",
       "      <th>Group</th>\n",
       "      <th>Visit Date</th>\n",
       "      <th>Brooke</th>\n",
       "      <th>HMFSE (max 66)</th>\n",
       "      <th>CHOP INTEND (max 64)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>V1</td>\n",
       "      <td>10.0</td>\n",
       "      <td>M</td>\n",
       "      <td>R</td>\n",
       "      <td>DMD</td>\n",
       "      <td>1/27/2021</td>\n",
       "      <td>1.0</td>\n",
       "      <td>44</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>V2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>R</td>\n",
       "      <td>DMD</td>\n",
       "      <td>7/8/2021</td>\n",
       "      <td>1.0</td>\n",
       "      <td>45</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>V3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>R</td>\n",
       "      <td>DMD</td>\n",
       "      <td>11/10/2021</td>\n",
       "      <td>1.0</td>\n",
       "      <td>40</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>V4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>R</td>\n",
       "      <td>DMD</td>\n",
       "      <td>8/15/2023</td>\n",
       "      <td>2.0</td>\n",
       "      <td>33</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>V5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>R</td>\n",
       "      <td>DMD</td>\n",
       "      <td>2/20/2024</td>\n",
       "      <td>3.0</td>\n",
       "      <td>24</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>67</td>\n",
       "      <td>V1</td>\n",
       "      <td>5.0</td>\n",
       "      <td>F</td>\n",
       "      <td>L</td>\n",
       "      <td>Healthy</td>\n",
       "      <td>3/29/2024</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>69</td>\n",
       "      <td>V1</td>\n",
       "      <td>16.0</td>\n",
       "      <td>M</td>\n",
       "      <td>R</td>\n",
       "      <td>DMD</td>\n",
       "      <td>5/9/2024</td>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>71</td>\n",
       "      <td>V1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>M</td>\n",
       "      <td>R</td>\n",
       "      <td>SMA</td>\n",
       "      <td>9/25/2024</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>71</td>\n",
       "      <td>V2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>R</td>\n",
       "      <td>SMA</td>\n",
       "      <td>3/18/2025</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>72</td>\n",
       "      <td>V1</td>\n",
       "      <td>5.0</td>\n",
       "      <td>F</td>\n",
       "      <td>R</td>\n",
       "      <td>SMA</td>\n",
       "      <td>1/25/2025</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>146 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Subj# Visit #  Age @ \\nenrollment Gender Dom    Group  Visit Date  \\\n",
       "0        1      V1                10.0      M   R      DMD   1/27/2021   \n",
       "1        1      V2                 NaN      M   R      DMD    7/8/2021   \n",
       "2        1      V3                 NaN      M   R      DMD  11/10/2021   \n",
       "3        1      V4                 NaN     M    R      DMD   8/15/2023   \n",
       "4        1      V5                 NaN      M   R      DMD   2/20/2024   \n",
       "..     ...     ...                 ...    ...  ..      ...         ...   \n",
       "141     67      V1                 5.0      F   L  Healthy   3/29/2024   \n",
       "142     69      V1                16.0      M   R      DMD    5/9/2024   \n",
       "143     71      V1                 2.0      M   R      SMA   9/25/2024   \n",
       "144     71      V2                 NaN      M   R      SMA   3/18/2025   \n",
       "145     72      V1                 5.0      F   R      SMA   1/25/2025   \n",
       "\n",
       "     Brooke HMFSE (max 66) CHOP INTEND (max 64)  \n",
       "0       1.0             44                  NaN  \n",
       "1       1.0             45                  NaN  \n",
       "2       1.0             40                  NaN  \n",
       "3       2.0             33                  NaN  \n",
       "4       3.0             24                  NaN  \n",
       "..      ...            ...                  ...  \n",
       "141     1.0            NaN                  NaN  \n",
       "142     5.0            NaN                  NaN  \n",
       "143     1.0            NaN                  NaN  \n",
       "144     1.0            NaN                  NaN  \n",
       "145     1.0            NaN                  NaN  \n",
       "\n",
       "[146 rows x 10 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brooke_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ead3b832-c4b4-4ae0-a70a-e4acb6167318",
   "metadata": {},
   "outputs": [],
   "source": [
    "# video_dir = Path(\"/mnt/sdb/arafat/U-Extend/IEEE_Codes/videos\")\n",
    "\n",
    "# for index, row in brooke_df.iterrows():\n",
    "#     subj = str(row['Subj#']).strip()\n",
    "#     visit_date = str(row['Visit Date']).strip()\n",
    "#     visit_date_clean = pd.to_datetime(visit_date).strftime('%m-%d-%y')\n",
    "\n",
    "#     print(subj, visit_date_clean)\n",
    "    \n",
    "#     video_path = Path(os.path.join(video_dir, subj, visit_date_clean,'segments'))\n",
    "#     mp4_files = list(video_path.glob(\"*.mp4\"))\n",
    "    \n",
    "#     savepath = Path(os.path.join(video_dir, subj, visit_date_clean,'2D'))\n",
    "#     savepath.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "#     for f in mp4_files:\n",
    "#         subprocess.run([\n",
    "#         './scripts/inference_new.sh',\n",
    "#         './configs/halpe_26/resnet/256x192_res50_lr1e-3_1x.yaml',\n",
    "#         './pretrained_models/halpe26_fast_res50_256x192.pth',\n",
    "#         f,\n",
    "#         savepath])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f2b183a9-787a-4f85-8023-a991d96de8e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Subj#</th>\n",
       "      <th>Visit #</th>\n",
       "      <th>Age @ \\nenrollment</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Dom</th>\n",
       "      <th>Group</th>\n",
       "      <th>Visit Date</th>\n",
       "      <th>Brooke</th>\n",
       "      <th>HMFSE (max 66)</th>\n",
       "      <th>CHOP INTEND (max 64)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>V3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>R</td>\n",
       "      <td>DMD</td>\n",
       "      <td>11/10/2021</td>\n",
       "      <td>1.0</td>\n",
       "      <td>40</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Subj# Visit #  Age @ \\nenrollment Gender Dom Group  Visit Date  Brooke  \\\n",
       "2      1      V3                 NaN      M   R   DMD  11/10/2021     1.0   \n",
       "\n",
       "  HMFSE (max 66) CHOP INTEND (max 64)  \n",
       "2             40                  NaN  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = brooke_df[brooke_df['Subj#'] == 1].reset_index(drop=True)\n",
    "df = df.iloc[2:3,:]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bab4f4f4-60ab-4268-aa10-4785f911cb1b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 11-10-21\n",
      "[PosixPath('/mnt/sdb/arafat/U-Extend/IEEE_Codes/videos/1/11-10-21/segments/1_lext_2.mp4'), PosixPath('/mnt/sdb/arafat/U-Extend/IEEE_Codes/videos/1/11-10-21/segments/1_rext_2.mp4'), PosixPath('/mnt/sdb/arafat/U-Extend/IEEE_Codes/videos/1/11-10-21/segments/1_rflex_2.mp4'), PosixPath('/mnt/sdb/arafat/U-Extend/IEEE_Codes/videos/1/11-10-21/segments/1_lflex_2.mp4'), PosixPath('/mnt/sdb/arafat/U-Extend/IEEE_Codes/videos/1/11-10-21/segments/1_lext_1.mp4'), PosixPath('/mnt/sdb/arafat/U-Extend/IEEE_Codes/videos/1/11-10-21/segments/1_lflex_1.mp4'), PosixPath('/mnt/sdb/arafat/U-Extend/IEEE_Codes/videos/1/11-10-21/segments/1_rflex_1.mp4'), PosixPath('/mnt/sdb/arafat/U-Extend/IEEE_Codes/videos/1/11-10-21/segments/1_rext_1.mp4')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "+ CONFIG=./configs/halpe_26/resnet/256x192_res50_lr1e-3_1x.yaml\n",
      "+ CKPT=./pretrained_models/halpe26_fast_res50_256x192.pth\n",
      "+ VIDEO=/mnt/sdb/arafat/U-Extend/IEEE_Codes/videos/1/11-10-21/segments/1_lext_2.mp4\n",
      "+ OUTDIR=/mnt/sdb/arafat/U-Extend/IEEE_Codes/videos/1/11-10-21/2D\n",
      "+ python scripts/demo_inference_new.py --cfg ./configs/halpe_26/resnet/256x192_res50_lr1e-3_1x.yaml --checkpoint ./pretrained_models/halpe26_fast_res50_256x192.pth --video /mnt/sdb/arafat/U-Extend/IEEE_Codes/videos/1/11-10-21/segments/1_lext_2.mp4 --outdir /mnt/sdb/arafat/U-Extend/IEEE_Codes/videos/1/11-10-21/2D --detector yolo --save_video\n",
      "2026-01-22 17:00:15 [DEBUG]: Loaded backend agg version v2.2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/sdb/arafat/U-Extend/IEEE_Codes/videos/1/11-10-21/segments/1_lext_2.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/sdb/arafat/.local/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/mnt/sdb/arafat/.local/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "/mnt/sdb/arafat/U-Extend/AlphaPose/scripts/demo_inference_new.py:187: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  pose_model.load_state_dict(torch.load(args.checkpoint, map_location=args.device))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pose model from ./pretrained_models/halpe26_fast_res50_256x192.pth...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/41 [00:00<?, ?it/s]2026-01-22 17:00:19 [DEBUG]: Loaded backend agg version v2.2.\n",
      "2026-01-22 17:00:19 [DEBUG]: Loaded backend agg version v2.2.\n",
      "2026-01-22 17:00:19 [DEBUG]: Loaded backend agg version v2.2.\n",
      "2026-01-22 17:00:22 [DEBUG]: Loaded backend agg version v2.2.\n",
      "OpenCV: FFMPEG: tag 0x34504d46/'FMP4' is not supported with codec id 12 and format 'mp4 / MP4 (MPEG-4 Part 14)'\n",
      "OpenCV: FFMPEG: fallback to use tag 0x7634706d/'mp4v'\n",
      " 93%|█████████▎| 38/41 [00:05<00:00, 20.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading YOLO model..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 41/41 [00:06<00:00,  6.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results have been written to json.\n",
      "===========================> Finish Model Running.\n",
      "===========================> Rendering remaining images in the queue...\n",
      "===========================> If this step takes too long, you can enable the --vis_fast flag to use fast rendering (real-time).\n",
      "===========================> Rendering remaining 0 images in the queue..."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "+ CONFIG=./configs/halpe_26/resnet/256x192_res50_lr1e-3_1x.yaml\n",
      "+ CKPT=./pretrained_models/halpe26_fast_res50_256x192.pth\n",
      "+ VIDEO=/mnt/sdb/arafat/U-Extend/IEEE_Codes/videos/1/11-10-21/segments/1_rext_2.mp4\n",
      "+ OUTDIR=/mnt/sdb/arafat/U-Extend/IEEE_Codes/videos/1/11-10-21/2D\n",
      "+ python scripts/demo_inference_new.py --cfg ./configs/halpe_26/resnet/256x192_res50_lr1e-3_1x.yaml --checkpoint ./pretrained_models/halpe26_fast_res50_256x192.pth --video /mnt/sdb/arafat/U-Extend/IEEE_Codes/videos/1/11-10-21/segments/1_rext_2.mp4 --outdir /mnt/sdb/arafat/U-Extend/IEEE_Codes/videos/1/11-10-21/2D --detector yolo --save_video\n",
      "2026-01-22 17:00:30 [DEBUG]: Loaded backend agg version v2.2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/sdb/arafat/U-Extend/IEEE_Codes/videos/1/11-10-21/segments/1_rext_2.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/sdb/arafat/.local/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/mnt/sdb/arafat/.local/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "/mnt/sdb/arafat/U-Extend/AlphaPose/scripts/demo_inference_new.py:187: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  pose_model.load_state_dict(torch.load(args.checkpoint, map_location=args.device))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pose model from ./pretrained_models/halpe26_fast_res50_256x192.pth...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/41 [00:00<?, ?it/s]2026-01-22 17:00:33 [DEBUG]: Loaded backend agg version v2.2.\n",
      "2026-01-22 17:00:33 [DEBUG]: Loaded backend agg version v2.2.\n",
      "2026-01-22 17:00:34 [DEBUG]: Loaded backend agg version v2.2.\n",
      "2026-01-22 17:00:36 [DEBUG]: Loaded backend agg version v2.2.\n",
      "OpenCV: FFMPEG: tag 0x34504d46/'FMP4' is not supported with codec id 12 and format 'mp4 / MP4 (MPEG-4 Part 14)'\n",
      "OpenCV: FFMPEG: fallback to use tag 0x7634706d/'mp4v'\n",
      "100%|██████████| 41/41 [00:05<00:00,  7.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading YOLO model..\n",
      "Results have been written to json.\n",
      "===========================> Finish Model Running.\n",
      "===========================> Rendering remaining images in the queue...\n",
      "===========================> If this step takes too long, you can enable the --vis_fast flag to use fast rendering (real-time).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "+ CONFIG=./configs/halpe_26/resnet/256x192_res50_lr1e-3_1x.yaml\n",
      "+ CKPT=./pretrained_models/halpe26_fast_res50_256x192.pth\n",
      "+ VIDEO=/mnt/sdb/arafat/U-Extend/IEEE_Codes/videos/1/11-10-21/segments/1_rflex_2.mp4\n",
      "+ OUTDIR=/mnt/sdb/arafat/U-Extend/IEEE_Codes/videos/1/11-10-21/2D\n",
      "+ python scripts/demo_inference_new.py --cfg ./configs/halpe_26/resnet/256x192_res50_lr1e-3_1x.yaml --checkpoint ./pretrained_models/halpe26_fast_res50_256x192.pth --video /mnt/sdb/arafat/U-Extend/IEEE_Codes/videos/1/11-10-21/segments/1_rflex_2.mp4 --outdir /mnt/sdb/arafat/U-Extend/IEEE_Codes/videos/1/11-10-21/2D --detector yolo --save_video\n",
      "2026-01-22 17:00:43 [DEBUG]: Loaded backend agg version v2.2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/sdb/arafat/U-Extend/IEEE_Codes/videos/1/11-10-21/segments/1_rflex_2.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/sdb/arafat/.local/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/mnt/sdb/arafat/.local/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "/mnt/sdb/arafat/U-Extend/AlphaPose/scripts/demo_inference_new.py:187: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  pose_model.load_state_dict(torch.load(args.checkpoint, map_location=args.device))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pose model from ./pretrained_models/halpe26_fast_res50_256x192.pth...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/30 [00:00<?, ?it/s]2026-01-22 17:00:47 [DEBUG]: Loaded backend agg version v2.2.\n",
      "2026-01-22 17:00:47 [DEBUG]: Loaded backend agg version v2.2.\n",
      "2026-01-22 17:00:47 [DEBUG]: Loaded backend agg version v2.2.\n",
      "2026-01-22 17:00:50 [DEBUG]: Loaded backend agg version v2.2.\n",
      "OpenCV: FFMPEG: tag 0x34363268/'h264' is not supported with codec id 27 and format 'mp4 / MP4 (MPEG-4 Part 14)'\n",
      "OpenCV: FFMPEG: fallback to use tag 0x31637661/'avc1'\n",
      "[ERROR:0@1.957] global cap_ffmpeg_impl.hpp:3207 open Could not find encoder for codec_id=27, error: Encoder not found\n",
      "[ERROR:0@1.957] global cap_ffmpeg_impl.hpp:3285 open VIDEOIO/FFMPEG: Failed to initialize VideoWriter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading YOLO model..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:05<00:00,  5.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Try to use other video encoders...\n",
      "Results have been written to json.\n",
      "===========================> Finish Model Running.\n",
      "===========================> Rendering remaining images in the queue...\n",
      "===========================> If this step takes too long, you can enable the --vis_fast flag to use fast rendering (real-time).\n",
      "===========================> Rendering remaining 0 images in the queue..."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "+ CONFIG=./configs/halpe_26/resnet/256x192_res50_lr1e-3_1x.yaml\n",
      "+ CKPT=./pretrained_models/halpe26_fast_res50_256x192.pth\n",
      "+ VIDEO=/mnt/sdb/arafat/U-Extend/IEEE_Codes/videos/1/11-10-21/segments/1_lflex_2.mp4\n",
      "+ OUTDIR=/mnt/sdb/arafat/U-Extend/IEEE_Codes/videos/1/11-10-21/2D\n",
      "+ python scripts/demo_inference_new.py --cfg ./configs/halpe_26/resnet/256x192_res50_lr1e-3_1x.yaml --checkpoint ./pretrained_models/halpe26_fast_res50_256x192.pth --video /mnt/sdb/arafat/U-Extend/IEEE_Codes/videos/1/11-10-21/segments/1_lflex_2.mp4 --outdir /mnt/sdb/arafat/U-Extend/IEEE_Codes/videos/1/11-10-21/2D --detector yolo --save_video\n",
      "2026-01-22 17:00:57 [DEBUG]: Loaded backend agg version v2.2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/sdb/arafat/U-Extend/IEEE_Codes/videos/1/11-10-21/segments/1_lflex_2.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/sdb/arafat/.local/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/mnt/sdb/arafat/.local/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "/mnt/sdb/arafat/U-Extend/AlphaPose/scripts/demo_inference_new.py:187: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  pose_model.load_state_dict(torch.load(args.checkpoint, map_location=args.device))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pose model from ./pretrained_models/halpe26_fast_res50_256x192.pth...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/30 [00:00<?, ?it/s]2026-01-22 17:01:00 [DEBUG]: Loaded backend agg version v2.2.\n",
      "2026-01-22 17:01:01 [DEBUG]: Loaded backend agg version v2.2.\n",
      "2026-01-22 17:01:01 [DEBUG]: Loaded backend agg version v2.2.\n",
      "2026-01-22 17:01:03 [DEBUG]: Loaded backend agg version v2.2.\n",
      "OpenCV: FFMPEG: tag 0x34363268/'h264' is not supported with codec id 27 and format 'mp4 / MP4 (MPEG-4 Part 14)'\n",
      "OpenCV: FFMPEG: fallback to use tag 0x31637661/'avc1'\n",
      "[ERROR:0@1.949] global cap_ffmpeg_impl.hpp:3207 open Could not find encoder for codec_id=27, error: Encoder not found\n",
      "[ERROR:0@1.949] global cap_ffmpeg_impl.hpp:3285 open VIDEOIO/FFMPEG: Failed to initialize VideoWriter\n",
      " 33%|███▎      | 10/30 [00:04<00:06,  3.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading YOLO model..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:04<00:00,  6.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Try to use other video encoders...\n",
      "Results have been written to json.\n",
      "===========================> Finish Model Running.\n",
      "===========================> Rendering remaining images in the queue...\n",
      "===========================> If this step takes too long, you can enable the --vis_fast flag to use fast rendering (real-time).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "+ CONFIG=./configs/halpe_26/resnet/256x192_res50_lr1e-3_1x.yaml\n",
      "+ CKPT=./pretrained_models/halpe26_fast_res50_256x192.pth\n",
      "+ VIDEO=/mnt/sdb/arafat/U-Extend/IEEE_Codes/videos/1/11-10-21/segments/1_lext_1.mp4\n",
      "+ OUTDIR=/mnt/sdb/arafat/U-Extend/IEEE_Codes/videos/1/11-10-21/2D\n",
      "+ python scripts/demo_inference_new.py --cfg ./configs/halpe_26/resnet/256x192_res50_lr1e-3_1x.yaml --checkpoint ./pretrained_models/halpe26_fast_res50_256x192.pth --video /mnt/sdb/arafat/U-Extend/IEEE_Codes/videos/1/11-10-21/segments/1_lext_1.mp4 --outdir /mnt/sdb/arafat/U-Extend/IEEE_Codes/videos/1/11-10-21/2D --detector yolo --save_video\n",
      "2026-01-22 17:01:09 [DEBUG]: Loaded backend agg version v2.2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/sdb/arafat/U-Extend/IEEE_Codes/videos/1/11-10-21/segments/1_lext_1.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/sdb/arafat/.local/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/mnt/sdb/arafat/.local/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "/mnt/sdb/arafat/U-Extend/AlphaPose/scripts/demo_inference_new.py:187: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  pose_model.load_state_dict(torch.load(args.checkpoint, map_location=args.device))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pose model from ./pretrained_models/halpe26_fast_res50_256x192.pth...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/34 [00:00<?, ?it/s]2026-01-22 17:01:13 [DEBUG]: Loaded backend agg version v2.2.\n",
      "2026-01-22 17:01:13 [DEBUG]: Loaded backend agg version v2.2.\n",
      "2026-01-22 17:01:13 [DEBUG]: Loaded backend agg version v2.2.\n",
      " 26%|██▋       | 9/34 [00:03<00:07,  3.30it/s]2026-01-22 17:01:15 [DEBUG]: Loaded backend agg version v2.2.\n",
      "OpenCV: FFMPEG: tag 0x34504d46/'FMP4' is not supported with codec id 12 and format 'mp4 / MP4 (MPEG-4 Part 14)'\n",
      "OpenCV: FFMPEG: fallback to use tag 0x7634706d/'mp4v'\n",
      "100%|██████████| 34/34 [00:04<00:00,  8.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading YOLO model..\n",
      "Results have been written to json.\n",
      "===========================> Finish Model Running.\n",
      "===========================> Rendering remaining images in the queue...\n",
      "===========================> If this step takes too long, you can enable the --vis_fast flag to use fast rendering (real-time).\n",
      "===========================> Rendering remaining 0 images in the queue..."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "+ CONFIG=./configs/halpe_26/resnet/256x192_res50_lr1e-3_1x.yaml\n",
      "+ CKPT=./pretrained_models/halpe26_fast_res50_256x192.pth\n",
      "+ VIDEO=/mnt/sdb/arafat/U-Extend/IEEE_Codes/videos/1/11-10-21/segments/1_lflex_1.mp4\n",
      "+ OUTDIR=/mnt/sdb/arafat/U-Extend/IEEE_Codes/videos/1/11-10-21/2D\n",
      "+ python scripts/demo_inference_new.py --cfg ./configs/halpe_26/resnet/256x192_res50_lr1e-3_1x.yaml --checkpoint ./pretrained_models/halpe26_fast_res50_256x192.pth --video /mnt/sdb/arafat/U-Extend/IEEE_Codes/videos/1/11-10-21/segments/1_lflex_1.mp4 --outdir /mnt/sdb/arafat/U-Extend/IEEE_Codes/videos/1/11-10-21/2D --detector yolo --save_video\n",
      "2026-01-22 17:01:21 [DEBUG]: Loaded backend agg version v2.2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/sdb/arafat/U-Extend/IEEE_Codes/videos/1/11-10-21/segments/1_lflex_1.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/sdb/arafat/.local/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/mnt/sdb/arafat/.local/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "/mnt/sdb/arafat/U-Extend/AlphaPose/scripts/demo_inference_new.py:187: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  pose_model.load_state_dict(torch.load(args.checkpoint, map_location=args.device))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pose model from ./pretrained_models/halpe26_fast_res50_256x192.pth...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/33 [00:00<?, ?it/s]2026-01-22 17:01:25 [DEBUG]: Loaded backend agg version v2.2.\n",
      "2026-01-22 17:01:25 [DEBUG]: Loaded backend agg version v2.2.\n",
      "2026-01-22 17:01:25 [DEBUG]: Loaded backend agg version v2.2.\n",
      "2026-01-22 17:01:27 [DEBUG]: Loaded backend agg version v2.2.\n",
      "OpenCV: FFMPEG: tag 0x34504d46/'FMP4' is not supported with codec id 12 and format 'mp4 / MP4 (MPEG-4 Part 14)'\n",
      "OpenCV: FFMPEG: fallback to use tag 0x7634706d/'mp4v'\n",
      " 21%|██        | 7/33 [00:04<00:13,  1.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading YOLO model..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 33/33 [00:05<00:00,  6.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results have been written to json.\n",
      "===========================> Finish Model Running.\n",
      "===========================> Rendering remaining images in the queue...\n",
      "===========================> If this step takes too long, you can enable the --vis_fast flag to use fast rendering (real-time).\n",
      "===========================> Rendering remaining 0 images in the queue..."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "+ CONFIG=./configs/halpe_26/resnet/256x192_res50_lr1e-3_1x.yaml\n",
      "+ CKPT=./pretrained_models/halpe26_fast_res50_256x192.pth\n",
      "+ VIDEO=/mnt/sdb/arafat/U-Extend/IEEE_Codes/videos/1/11-10-21/segments/1_rflex_1.mp4\n",
      "+ OUTDIR=/mnt/sdb/arafat/U-Extend/IEEE_Codes/videos/1/11-10-21/2D\n",
      "+ python scripts/demo_inference_new.py --cfg ./configs/halpe_26/resnet/256x192_res50_lr1e-3_1x.yaml --checkpoint ./pretrained_models/halpe26_fast_res50_256x192.pth --video /mnt/sdb/arafat/U-Extend/IEEE_Codes/videos/1/11-10-21/segments/1_rflex_1.mp4 --outdir /mnt/sdb/arafat/U-Extend/IEEE_Codes/videos/1/11-10-21/2D --detector yolo --save_video\n",
      "2026-01-22 17:01:34 [DEBUG]: Loaded backend agg version v2.2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/sdb/arafat/U-Extend/IEEE_Codes/videos/1/11-10-21/segments/1_rflex_1.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/sdb/arafat/.local/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/mnt/sdb/arafat/.local/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "/mnt/sdb/arafat/U-Extend/AlphaPose/scripts/demo_inference_new.py:187: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  pose_model.load_state_dict(torch.load(args.checkpoint, map_location=args.device))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pose model from ./pretrained_models/halpe26_fast_res50_256x192.pth...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/34 [00:00<?, ?it/s]2026-01-22 17:01:38 [DEBUG]: Loaded backend agg version v2.2.\n",
      "2026-01-22 17:01:38 [DEBUG]: Loaded backend agg version v2.2.\n",
      "2026-01-22 17:01:38 [DEBUG]: Loaded backend agg version v2.2.\n",
      "2026-01-22 17:01:41 [DEBUG]: Loaded backend agg version v2.2.\n",
      "OpenCV: FFMPEG: tag 0x34504d46/'FMP4' is not supported with codec id 12 and format 'mp4 / MP4 (MPEG-4 Part 14)'\n",
      "OpenCV: FFMPEG: fallback to use tag 0x7634706d/'mp4v'\n",
      "100%|██████████| 34/34 [00:04<00:00,  7.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading YOLO model..\n",
      "Results have been written to json.\n",
      "===========================> Finish Model Running.\n",
      "===========================> Rendering remaining images in the queue...\n",
      "===========================> If this step takes too long, you can enable the --vis_fast flag to use fast rendering (real-time).\n",
      "===========================> Rendering remaining 0 images in the queue..."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "+ CONFIG=./configs/halpe_26/resnet/256x192_res50_lr1e-3_1x.yaml\n",
      "+ CKPT=./pretrained_models/halpe26_fast_res50_256x192.pth\n",
      "+ VIDEO=/mnt/sdb/arafat/U-Extend/IEEE_Codes/videos/1/11-10-21/segments/1_rext_1.mp4\n",
      "+ OUTDIR=/mnt/sdb/arafat/U-Extend/IEEE_Codes/videos/1/11-10-21/2D\n",
      "+ python scripts/demo_inference_new.py --cfg ./configs/halpe_26/resnet/256x192_res50_lr1e-3_1x.yaml --checkpoint ./pretrained_models/halpe26_fast_res50_256x192.pth --video /mnt/sdb/arafat/U-Extend/IEEE_Codes/videos/1/11-10-21/segments/1_rext_1.mp4 --outdir /mnt/sdb/arafat/U-Extend/IEEE_Codes/videos/1/11-10-21/2D --detector yolo --save_video\n",
      "2026-01-22 17:01:47 [DEBUG]: Loaded backend agg version v2.2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/sdb/arafat/U-Extend/IEEE_Codes/videos/1/11-10-21/segments/1_rext_1.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/sdb/arafat/.local/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/mnt/sdb/arafat/.local/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "/mnt/sdb/arafat/U-Extend/AlphaPose/scripts/demo_inference_new.py:187: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  pose_model.load_state_dict(torch.load(args.checkpoint, map_location=args.device))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pose model from ./pretrained_models/halpe26_fast_res50_256x192.pth...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/34 [00:00<?, ?it/s]2026-01-22 17:01:51 [DEBUG]: Loaded backend agg version v2.2.\n",
      "2026-01-22 17:01:51 [DEBUG]: Loaded backend agg version v2.2.\n",
      "2026-01-22 17:01:51 [DEBUG]: Loaded backend agg version v2.2.\n",
      "2026-01-22 17:01:54 [DEBUG]: Loaded backend agg version v2.2.\n",
      "OpenCV: FFMPEG: tag 0x34504d46/'FMP4' is not supported with codec id 12 and format 'mp4 / MP4 (MPEG-4 Part 14)'\n",
      "OpenCV: FFMPEG: fallback to use tag 0x7634706d/'mp4v'\n",
      " 21%|██        | 7/34 [00:04<00:11,  2.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading YOLO model..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 34/34 [00:04<00:00,  7.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results have been written to json.\n",
      "===========================> Finish Model Running.\n",
      "===========================> Rendering remaining images in the queue...\n",
      "===========================> If this step takes too long, you can enable the --vis_fast flag to use fast rendering (real-time).\n"
     ]
    }
   ],
   "source": [
    "video_dir = Path(\"/mnt/sdb/arafat/U-Extend/IEEE_Codes/videos\")\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    subj = str(row['Subj#']).strip()\n",
    "    visit_date = str(row['Visit Date']).strip()\n",
    "    visit_date_clean = pd.to_datetime(visit_date).strftime('%m-%d-%y')\n",
    "\n",
    "    print(subj, visit_date_clean)\n",
    "    \n",
    "    video_path = Path(os.path.join(video_dir, subj, visit_date_clean,'segments'))\n",
    "    mp4_files = list(video_path.glob(\"*.mp4\"))\n",
    "\n",
    "    print(mp4_files)\n",
    "\n",
    "    # r_only_paths = [p for p in mp4_files if p.stem.split('_')[1].startswith('r')]\n",
    "    \n",
    "    savepath = Path(os.path.join(video_dir, subj, visit_date_clean,'2D'))\n",
    "    savepath.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    for f in mp4_files:\n",
    "        subprocess.run([\n",
    "        './scripts/inference_new.sh',\n",
    "        './configs/halpe_26/resnet/256x192_res50_lr1e-3_1x.yaml',\n",
    "        './pretrained_models/halpe26_fast_res50_256x192.pth',\n",
    "        f,\n",
    "        savepath])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a5513a59-083a-4775-8500-8842a591a188",
   "metadata": {},
   "outputs": [],
   "source": [
    "# f = \"/mnt/sdb/arafat/U-Extend/IEEE_Codes/videos/47_problem/03-26-25_problem/segments/47_rext_1.mp4\"\n",
    "# savepath = \"/mnt/sdb/arafat/U-Extend/IEEE_Codes/videos/47_problem/03-26-25_problem/2D/\"\n",
    "\n",
    "# subprocess.run([\n",
    "#         './scripts/inference_new.sh',\n",
    "#         './configs/halpe_26/resnet/256x192_res50_lr1e-3_1x.yaml',\n",
    "#         './pretrained_models/halpe26_fast_res50_256x192.pth',\n",
    "#         f,\n",
    "#         savepath])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ff758480-fe2d-4652-b1ee-b00d739a105c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# savepath = '/mnt/sdb/arafat/U-Extend/'\n",
    "\n",
    "# f = '/mnt/sdb/arafat/U-Extend/test.mp4'\n",
    "# subprocess.run([\n",
    "#     './scripts/inference_new.sh',\n",
    "#     './configs/halpe_26/resnet/256x192_res50_lr1e-3_1x.yaml',\n",
    "#     './pretrained_models/halpe26_fast_res50_256x192.pth',\n",
    "#     f,\n",
    "#     savepath])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f77b1433-821e-49e0-9778-856ac466d2ca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
